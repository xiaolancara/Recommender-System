{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNUMINs8mRACQhWMFCcXKTt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xiaolancara/Recommender-System/blob/main/ncf_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "a_fzTRiO7nIK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import torch.nn.functional as F \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import torch.backends.cudnn as cudnn\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "import scipy.sparse as sp"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Config"
      ],
      "metadata": {
        "id": "anc9udSeBxxW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset name \n",
        "dataset = 'ml-1m'\n",
        "assert dataset in ['ml-1m', 'pinterest-20']\n",
        "\n",
        "# paths\n",
        "main_path = 'Data/'\n",
        "\n",
        "train_rating = main_path + '{}.train.rating'.format(dataset)\n",
        "test_rating = main_path + '{}.test.rating'.format(dataset)\n",
        "test_negative = main_path + '{}.test.negative'.format(dataset)\n",
        "\n"
      ],
      "metadata": {
        "id": "1_IaQW9dBwoe"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Dataset"
      ],
      "metadata": {
        "id": "vDM3UcY07oRy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_all(test_num=100):\n",
        "    \"\"\" We load all the three file here to save time in each epoch. \"\"\"\n",
        "    train_data = pd.read_csv(\n",
        "        train_rating, \n",
        "        sep='\\t', header=None, names=['user', 'item'], \n",
        "        usecols=[0, 1], dtype={0: np.int32, 1: np.int32})\n",
        "\n",
        "    user_num = train_data['user'].max() + 1\n",
        "    item_num = train_data['item'].max() + 1\n",
        "\n",
        "    train_data = train_data.values.tolist()\n",
        "\n",
        "    # load ratings as a dok matrix\n",
        "    train_mat = sp.dok_matrix((user_num, item_num), dtype=np.float32)\n",
        "    for x in train_data:\n",
        "        train_mat[x[0], x[1]] = 1.0\n",
        "\n",
        "    test_data = []\n",
        "    with open(test_negative, 'r') as fd:\n",
        "        line = fd.readline()\n",
        "        while line != None and line != '':\n",
        "            arr = line.split('\\t')\n",
        "            u = eval(arr[0])[0]\n",
        "            test_data.append([u, eval(arr[0])[1]])\n",
        "            for i in arr[1:]:\n",
        "                test_data.append([u, int(i)])\n",
        "            line = fd.readline()\n",
        "    return train_data, test_data, user_num, item_num, train_mat"
      ],
      "metadata": {
        "id": "OxvjNXUf8QX2"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, test_data, user_num ,item_num, train_mat = load_all()"
      ],
      "metadata": {
        "id": "O73wQv428Sb5"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Torch Dataset"
      ],
      "metadata": {
        "id": "mGCOlSw59bFo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NCFData(data.Dataset):\n",
        "    def __init__(self, features, \n",
        "                num_item, train_mat=None, num_ng=0, is_training=None):\n",
        "        super(NCFData, self).__init__()\n",
        "        \"\"\" Note that the labels are only useful when training, we thus \n",
        "            add them in the ng_sample() function.\n",
        "        \"\"\"\n",
        "        self.features_ps = features # positive cases\n",
        "        self.num_item = num_item\n",
        "        self.train_mat = train_mat\n",
        "        self.num_ng = num_ng \n",
        "        self.is_training = is_training\n",
        "        self.labels = [0 for _ in range(len(features))]\n",
        "    # for each pos, generate N+1 neg samples\n",
        "    def ng_sample(self):\n",
        "        assert self.is_training, 'no need to sampling when testing'\n",
        "\n",
        "        self.features_ng = []\n",
        "        for x in self.features_ps:\n",
        "            u = x[0]\n",
        "            for t in range(self.num_ng):\n",
        "                j = np.random.randint(self.num_item)\n",
        "                while (u, j) in self.train_mat:\n",
        "                    j = np.random.randint(self.num_item)\n",
        "                self.features_ng.append([u, j])\n",
        "\n",
        "        labels_ps = [1 for _ in range(len(self.features_ps))]\n",
        "        labels_ng = [0 for _ in range(len(self.features_ng))]\n",
        "\n",
        "        self.features_fill = self.features_ps + self.features_ng\n",
        "        self.labels_fill = labels_ps + labels_ng\n",
        "\n",
        "    def __len__(self):\n",
        "        # for each pos, generate N+1 neg samples\n",
        "        return (self.num_ng + 1) * len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        features = self.features_fill if self.is_training \\\n",
        "                    else self.features_ps\n",
        "        labels = self.labels_fill if self.is_training \\\n",
        "                    else self.labels\n",
        "\n",
        "        user = features[idx][0]\n",
        "        item = features[idx][1]\n",
        "        label = labels[idx]\n",
        "        return user, item ,label\n"
      ],
      "metadata": {
        "id": "pWX533vD8SeO"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_ng = 4\n",
        "train_dataset = NCFData(\n",
        "    train_data, item_num, train_mat, num_ng, True)\n"
      ],
      "metadata": {
        "id": "g-1UHch98Sgn"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = NCFData(\n",
        "    test_data, item_num, train_mat, 0, False)"
      ],
      "metadata": {
        "id": "cSHd5Av88QeF"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = data.DataLoader(train_dataset,\n",
        "    batch_size=256, shuffle=True, num_workers=4)\n",
        "test_loader = data.DataLoader(test_dataset,\n",
        "    batch_size=100, shuffle=False, num_workers=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7K1JsyV8Qgu",
        "outputId": "7c9a313e-7f52-415a-dfdd-3cf12ffadf13"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Model"
      ],
      "metadata": {
        "id": "6-NFu2IY8bR0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = './models/'\n",
        "GMF_model_path = model_path + 'GMF.pth'\n",
        "MLP_model_path = model_path + 'MLP.pth'\n",
        "NeuMF_model_path = model_path + 'NeuMF.pth'\n",
        "# model name \n",
        "model_name = 'NeuMF-end'\n",
        "assert model_name in ['MLP', 'GMF', 'NeuMF-end', 'NeuMF-pre']\n",
        "\n",
        "if model_name == 'NeuMF-pre':\n",
        "    assert os.path.exists(GMF_model_path), 'lack of GMF model'\n",
        "    assert os.path.exists(MLP_model_path), 'lack of MLP model'\n",
        "    GMF_model = torch.load(GMF_model_path)\n",
        "    MLP_model = torch.load(MLP_model_path)\n",
        "else:\n",
        "    GMF_model = None\n",
        "    MLP_model = None"
      ],
      "metadata": {
        "id": "8kMJ19HK8Zp8"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NCF(nn.Module):\n",
        "    def __init__(self, user_num, item_num, factor_num, num_layers,\n",
        "                    dropout, model, GMF_model=None, MLP_model=None):\n",
        "        super(NCF, self).__init__()\n",
        "        \"\"\"\n",
        "        user_num: number of users;\n",
        "        item_num: number of items;\n",
        "        factor_num: number of predictive factors;\n",
        "        num_layers: the number of layers in MLP model;\n",
        "        dropout: dropout rate between fully connected layers;\n",
        "        model: 'MLP', 'GMF', 'NeuMF-end', and 'NeuMF-pre';\n",
        "        GMF_model: pre-trained GMF weights;\n",
        "        MLP_model: pre-trained MLP weights.\n",
        "        \"\"\"\t\t\n",
        "        self.dropout = dropout\n",
        "        self.model = model\n",
        "        self.GMF_model = GMF_model\n",
        "        self.MLP_model = MLP_model\n",
        "\n",
        "        self.embed_user_GMF = nn.Embedding(user_num, factor_num)\n",
        "        self.embed_item_GMF = nn.Embedding(item_num, factor_num)\n",
        "        self.embed_user_MLP = nn.Embedding(\n",
        "                user_num, factor_num * (2 ** (num_layers - 1)))\n",
        "        self.embed_item_MLP = nn.Embedding(\n",
        "                item_num, factor_num * (2 ** (num_layers - 1)))\n",
        "\n",
        "        MLP_modules = []\n",
        "        for i in range(num_layers):\n",
        "            input_size = factor_num * (2 ** (num_layers - i))\n",
        "            MLP_modules.append(nn.Dropout(p=self.dropout))\n",
        "            MLP_modules.append(nn.Linear(input_size, input_size//2))\n",
        "            MLP_modules.append(nn.ReLU())\n",
        "        self.MLP_layers = nn.Sequential(*MLP_modules)\n",
        "\n",
        "        if self.model in ['MLP', 'GMF']:\n",
        "            predict_size = factor_num \n",
        "        else:\n",
        "            predict_size = factor_num * 2\n",
        "        self.predict_layer = nn.Linear(predict_size, 1)\n",
        "\n",
        "        self._init_weight_()\n",
        "\n",
        "    def _init_weight_(self):\n",
        "        \"\"\" We leave the weights initialization here. \"\"\"\n",
        "        if not self.model == 'NeuMF-pre':\n",
        "            nn.init.normal_(self.embed_user_GMF.weight, std=0.01)\n",
        "            nn.init.normal_(self.embed_user_MLP.weight, std=0.01)\n",
        "            nn.init.normal_(self.embed_item_GMF.weight, std=0.01)\n",
        "            nn.init.normal_(self.embed_item_MLP.weight, std=0.01)\n",
        "\n",
        "            for m in self.MLP_layers:\n",
        "                if isinstance(m, nn.Linear):\n",
        "                    nn.init.xavier_uniform_(m.weight)\n",
        "            nn.init.kaiming_uniform_(self.predict_layer.weight, \n",
        "                                    a=1, nonlinearity='sigmoid')\n",
        "\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                    m.bias.data.zero_()\n",
        "        else:\n",
        "            # embedding layers\n",
        "            self.embed_user_GMF.weight.data.copy_(\n",
        "                            self.GMF_model.embed_user_GMF.weight)\n",
        "            self.embed_item_GMF.weight.data.copy_(\n",
        "                            self.GMF_model.embed_item_GMF.weight)\n",
        "            self.embed_user_MLP.weight.data.copy_(\n",
        "                            self.MLP_model.embed_user_MLP.weight)\n",
        "            self.embed_item_MLP.weight.data.copy_(\n",
        "                            self.MLP_model.embed_item_MLP.weight)\n",
        "\n",
        "            # mlp layers\n",
        "            for (m1, m2) in zip(\n",
        "                self.MLP_layers, self.MLP_model.MLP_layers):\n",
        "                if isinstance(m1, nn.Linear) and isinstance(m2, nn.Linear):\n",
        "                    m1.weight.data.copy_(m2.weight)\n",
        "                    m1.bias.data.copy_(m2.bias)\n",
        "\n",
        "            # predict layers\n",
        "            predict_weight = torch.cat([\n",
        "                self.GMF_model.predict_layer.weight, \n",
        "                self.MLP_model.predict_layer.weight], dim=1)\n",
        "            precit_bias = self.GMF_model.predict_layer.bias + \\\n",
        "                        self.MLP_model.predict_layer.bias\n",
        "\n",
        "            self.predict_layer.weight.data.copy_(0.5 * predict_weight)\n",
        "            self.predict_layer.bias.data.copy_(0.5 * precit_bias)\n",
        "\n",
        "    def forward(self, user, item):\n",
        "        if not self.model == 'MLP':\n",
        "            embed_user_GMF = self.embed_user_GMF(user)\n",
        "            embed_item_GMF = self.embed_item_GMF(item)\n",
        "            output_GMF = embed_user_GMF * embed_item_GMF\n",
        "        if not self.model == 'GMF':\n",
        "            embed_user_MLP = self.embed_user_MLP(user)\n",
        "            embed_item_MLP = self.embed_item_MLP(item)\n",
        "            interaction = torch.cat((embed_user_MLP, embed_item_MLP), -1)\n",
        "            output_MLP = self.MLP_layers(interaction)\n",
        "\n",
        "        if self.model == 'GMF':\n",
        "            concat = output_GMF\n",
        "        elif self.model == 'MLP':\n",
        "            concat = output_MLP\n",
        "        else:\n",
        "            concat = torch.cat((output_GMF, output_MLP), -1)\n",
        "\n",
        "        prediction = self.predict_layer(concat)\n",
        "        return prediction.view(-1)\n"
      ],
      "metadata": {
        "id": "3iNQPclm8ZsJ"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "factor_num = 16\n",
        "num_layers = 3\n",
        "drop_out = 0\n",
        "learning_rate = 0.001\n",
        "model = NCF(user_num, item_num, factor_num, num_layers, \n",
        "                    drop_out, model_name, GMF_model, MLP_model)\n",
        "model.cuda()\n",
        "loss_function = nn.BCEWithLogitsLoss()\n",
        "\n",
        "if model == 'NeuMF-pre':\n",
        "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "else:\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "IV7FJrfz8ZuV"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "e34YlquUBK99"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def hit(gt_item, pred_items):\n",
        "\tif gt_item in pred_items:\n",
        "\t\treturn 1\n",
        "\treturn 0\n",
        "\n",
        "\n",
        "def ndcg(gt_item, pred_items):\n",
        "\tif gt_item in pred_items:\n",
        "\t\tindex = pred_items.index(gt_item)\n",
        "\t\treturn np.reciprocal(np.log2(index+2))\n",
        "\treturn 0\n",
        "\n",
        "\n",
        "def metrics(model, test_loader, top_k):\n",
        "\tHR, NDCG = [], []\n",
        "\n",
        "\tfor user, item, label in test_loader:\n",
        "\t\tuser = user.cuda()\n",
        "\t\titem = item.cuda()\n",
        "\n",
        "\t\tpredictions = model(user, item)\n",
        "\t\t_, indices = torch.topk(predictions, top_k)\n",
        "\t\trecommends = torch.take(\n",
        "\t\t\t\titem, indices).cpu().numpy().tolist()\n",
        "\n",
        "\t\tgt_item = item[0].item()\n",
        "\t\tHR.append(hit(gt_item, recommends))\n",
        "\t\tNDCG.append(ndcg(gt_item, recommends))\n",
        "\n",
        "\treturn np.mean(HR), np.mean(NDCG)"
      ],
      "metadata": {
        "id": "qyRvWrv5BJVd"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "EY8ALpYH9O88"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "count, best_hr = 0, 0\n",
        "epochs = 2\n",
        "top_k = 10\n",
        "out = True\n",
        "for epoch in range(epochs):\n",
        "\tmodel.train() # Enable dropout (if have).\n",
        "\tstart_time = time.time()\n",
        "\ttrain_loader.dataset.ng_sample()\n",
        "\n",
        "\tfor user, item, label in train_loader:\n",
        "\t\tuser = user.cuda()\n",
        "\t\titem = item.cuda()\n",
        "\t\tlabel = label.float().cuda()\n",
        "\n",
        "\t\tmodel.zero_grad()\n",
        "\t\tprediction = model(user, item)\n",
        "\t\tloss = loss_function(prediction, label)\n",
        "\t\tloss.backward()\n",
        "\t\toptimizer.step()\n",
        "\t\t# writer.add_scalar('data/loss', loss.item(), count)\n",
        "\t\tcount += 1\n",
        "\n",
        "\tmodel.eval()\n",
        "\tHR, NDCG = metrics(model, test_loader, top_k)\n",
        "\n",
        "\telapsed_time = time.time() - start_time\n",
        "\tprint(\"The time elapse of epoch {:03d}\".format(epoch) + \" is: \" + \n",
        "\t\t\ttime.strftime(\"%H: %M: %S\", time.gmtime(elapsed_time)))\n",
        "\tprint(\"HR: {:.3f}\\tNDCG: {:.3f}\".format(np.mean(HR), np.mean(NDCG)))\n",
        "\n",
        "\tif HR > best_hr:\n",
        "\t\tbest_hr, best_ndcg, best_epoch = HR, NDCG, epoch\n",
        "\t\tif out:\n",
        "\t\t\tif not os.path.exists(model_path):\n",
        "\t\t\t\tos.mkdir(model_path)\n",
        "\t\t\ttorch.save(model, \n",
        "\t\t\t\t'{}{}.pth'.format(model_path, model_name))\n",
        "\n",
        "print(\"End. Best epoch {:03d}: HR = {:.3f}, NDCG = {:.3f}\".format(\n",
        "\t\t\t\t\t\t\t\t\tbest_epoch, best_hr, best_ndcg))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pY8OsgKu8ZzE",
        "outputId": "db819fc8-114d-48c6-8f55-538e0328a303"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The time elapse of epoch 000 is: 00: 02: 07\n",
            "HR: 0.609\tNDCG: 0.347\n",
            "The time elapse of epoch 001 is: 00: 02: 09\n",
            "HR: 0.656\tNDCG: 0.381\n",
            "End. Best epoch 001: HR = 0.656, NDCG = 0.381\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SzvzGIjj-KbD"
      },
      "execution_count": 26,
      "outputs": []
    }
  ]
}